#!/usr/bin/env python3
"""
Preload sentence transformer models for offline use.
Run this when you have internet access to cache models locally.
"""

import os
import sys
from pathlib import Path

def preload_models():
    """Download and cache sentence transformer models"""
    try:
        from sentence_transformers import SentenceTransformer
        import torch
        
        print("ğŸ¤– Preloading AI models for offline use...")
        
        # Create models directory
        models_dir = Path(".venv/models")
        models_dir.mkdir(exist_ok=True)
        
        # Lightweight models that work well for code similarity
        models_to_cache = [
            "all-MiniLM-L6-v2",  # Fast, good general purpose
            "all-mpnet-base-v2", # Better quality, slightly larger
        ]
        
        for model_name in models_to_cache:
            print(f"ğŸ“¦ Downloading {model_name}...")
            try:
                # Load model (this caches it automatically)
                model = SentenceTransformer(model_name)
                
                # Test encoding to ensure it works
                test_text = ["This is a test sentence.", "This is another test."]
                embeddings = model.encode(test_text)
                
                print(f"âœ… {model_name} cached successfully ({embeddings.shape})")
                
            except Exception as e:
                print(f"âŒ Failed to cache {model_name}: {e}")
        
        print("\nğŸ‰ Model preloading complete!")
        print("ğŸ’¡ Models are cached in ~/.cache/torch/sentence_transformers/")
        print("ğŸ’¡ The application will work offline now for sentence embeddings.")
        
        return True
        
    except ImportError as e:
        print(f"âŒ Required packages not installed: {e}")
        print("ğŸ“¦ Run: .venv/Scripts/python.exe -m pip install sentence-transformers")
        return False
    except Exception as e:
        print(f"âŒ Error preloading models: {e}")
        return False

def check_cache():
    """Check what models are already cached"""
    try:
        import torch
        from sentence_transformers import SentenceTransformer
        
        cache_dir = Path.home() / ".cache" / "torch" / "sentence_transformers"
        if cache_dir.exists():
            cached_models = [d.name for d in cache_dir.iterdir() if d.is_dir()]
            if cached_models:
                print("ğŸ“‹ Cached models found:")
                for model in cached_models:
                    print(f"  âœ… {model}")
            else:
                print("ğŸ“‹ No cached models found")
        else:
            print("ğŸ“‹ No cache directory found")
            
    except Exception as e:
        print(f"âŒ Error checking cache: {e}")

if __name__ == "__main__":
    print("ğŸ” Checking current model cache...")
    check_cache()
    print()
    
    if len(sys.argv) > 1 and sys.argv[1] == "--check-only":
        print("ğŸ Check complete.")
    else:
        success = preload_models()
        if success:
            print("\nğŸ” Final cache status:")
            check_cache()
        else:
            print("\nâŒ Preloading failed. Models will be downloaded when first used.")
            sys.exit(1) 